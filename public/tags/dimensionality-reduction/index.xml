<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dimensionality Reduction on </title>
    <link>http://localhost:1313/tags/dimensionality-reduction/</link>
    <description>Recent content in Dimensionality Reduction on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Jul 2024 19:04:00 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/dimensionality-reduction/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pricipal Component Analyisi (PCA)</title>
      <link>http://localhost:1313/posts/classical_ml/pca/</link>
      <pubDate>Sun, 28 Jul 2024 19:04:00 +0200</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/pca/</guid>
      <description>Introduction Principal Component Analysis (PCA) is an unsupervised Machine Learning algorithm for dimensionality reduction. In Data Science and Machine Learning often huge datasets with a large set of features are analysed. PCA allows to simplify complex datasets while retaining their essential information. PCA transforms a large set of correlated variables into a smaller set of uncorrelated variables called principal components. These principal components capture the maximum variance in the data, making it easier to identify patterns, reduce noise, and improve the efficiency of machine learning models.</description>
    </item>
  </channel>
</rss>
