<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on </title>
    <link>http://localhost:1313/tags/data-science/</link>
    <description>Recent content in Data Science on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2024 02:41:59 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/data-science/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding Recurrent Neural Networks (RNN)</title>
      <link>http://localhost:1313/posts/deep_learning/rnn/</link>
      <pubDate>Mon, 21 Oct 2024 02:41:59 +0200</pubDate>
      <guid>http://localhost:1313/posts/deep_learning/rnn/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Recurrent Neural Networks (RNNs) are a specific type of Neural Networks (NNs) that are especially relevant for sequential data like time series, text, or audio data. In standard NNs all data is treated independently. For example they are not able to capture the temporal relationship in a time series. RNNs however, process the data sequentially to remember data from the past.&lt;/p&gt;&#xA;&lt;h2 id=&#34;rnn-architecture&#34;&gt;RNN Architecture&lt;/h2&gt;&#xA;&lt;p&gt;In a standard NN all data is processed in parallel. As discussed in &lt;a href=&#34;http://localhost:1313/posts/deep_learning/intro_dl/&#34; title=&#34;intro_dl&#34;&gt;Introduction to Deep Learning&lt;/a&gt; we have an input layer, an output layer and in between a set of hidden layers. All the outputs are calculated independently and there is no connection beween them. A RNN in contrast uses the output of one step as input of the next step in addition to the input data and in that way creates a connection and a memory to data of previous steps. The difference in the architecture is illustrated in the following plot.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Principal Component Analysis (PCA)</title>
      <link>http://localhost:1313/posts/classical_ml/pca/</link>
      <pubDate>Wed, 09 Oct 2024 19:04:00 +0200</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/pca/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Principal Component Analysis (PCA) is an unsupervised Machine Learning algorithm for dimensionality reduction. In Data Science and Machine Learning, large datasets with numerous features are often analyzed. PCA simplifies these complex datasets by retaining their essential information while reducing their dimensionality. It transforms a large set of correlated variables into a smaller set of uncorrelated variables known as &lt;em&gt;principal components&lt;/em&gt;. These principal components capture the maximum variance in the data. They are ordered in decreasing order of explaining variance. This makes it easier to identify patterns, reduce noise, and enhance the efficiency of Machine Learning models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding K-Means Clustering</title>
      <link>http://localhost:1313/posts/classical_ml/kmeans/</link>
      <pubDate>Thu, 13 Jun 2024 21:10:46 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/kmeans/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;K-Means&lt;/em&gt; is an example of a &lt;em&gt;clustering&lt;/em&gt; algorithm. Clustering is a fundamental concept in Machine Learning, where the goal is to group a set of objects so that objects in the same group are more similar to each other than to those in other groups. Clustering belongs to the set of &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/supervised_unsupervised/#unsupervised&#34;&gt;unsupervised&lt;/a&gt; Machine Learning algorithms, that is no ground truth is needed. Among the various clustering algorithms, &lt;em&gt;K-Means&lt;/em&gt; stands out for its simplicity and efficiency. In this blog post, we will explain the algorithm behind K-Means, and see how to implement it in Python.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Boosting Variants - Sklearn vs. XGBoost vs. LightGBM vs. CatBoost</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_variants/</link>
      <pubDate>Wed, 08 May 2024 20:55:43 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_variants/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Gradient Boosting is an ensemble model of a sequential series of shallow &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_trees/&#34;&gt;Decision Trees&lt;/a&gt;. The single trees are weak learners with little predictive skill, but together, they form a strong learner with high predictive skill. For a more detailed explanation, please refer to the post &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_regression/&#34;&gt;Gradient Boosting for Regression - Explained&lt;/a&gt;. In this article, we will discuss different implementations of Gradient Boosting. The focus is to give a high-level overview of different implementations and discuss the differences. For a more in-depth understanding of each framework, further literature is given.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Boost for Classification Example</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_classification_example/</link>
      <pubDate>Sun, 28 Apr 2024 17:01:32 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_classification_example/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In this post, we develop a Gradient Boosting model for a binary classification. We focus on the calculations of each single step for a specific example chosen. For a more general explanation of the algorithm and the derivation of the formulas for the individual steps, please refer to &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_classification/&#34;&gt;Gradient Boost for Classification - Explained&lt;/a&gt; and &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_regression/&#34;&gt;Gradient Boost for Regression - Explained&lt;/a&gt;. Additionally, we show a simple example of how to apply Gradient Boosting for classification in Python.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Boost for Classification - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_classification/</link>
      <pubDate>Sun, 14 Apr 2024 20:45:19 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_classification/</guid>
      <description>&lt;hr&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Gradient Boosting is an &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/ensemble/&#34;&gt;ensemble&lt;/a&gt; machine learning model, that - as the name suggests - is based on &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/ensemble/#boosting&#34;&gt;boosting&lt;/a&gt;. An ensemble model based on boosting refers to a model that sequentially builds models, and the new model depends on the previous model. In Gradient Boosting these models are built such that they improve the error of the previous model. These individual models are so-called weak learners, which means they have low predictive skills. The ensemble of these weak learners builds the final model, which is a strong learner with a high predictive skill. In this post, we go through the algorithm of Gradient Boosting in general and then concretize the individual steps for a classification task using &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_trees/&#34;&gt;Decision Trees&lt;/a&gt; as weak learners and the log-loss function. There will be some overlapping with the article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_regression/&#34;&gt;Gradient Boosting for Regression - Explained&lt;/a&gt;, where a detailed explanation of Gradient Boosting is given, which is then applied to a regression problem. However, in this article, do not go into the details of the general formulation, for that please refer to the previously mentioned post. If you are interested in a concrete example with detailed calculations, please refer to &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_regression_example/&#34;&gt;Gradient Boosting for Regression - Example&lt;/a&gt; for a regression problem and &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_classification_example/&#34;&gt;Gradient Boosting for Classification - Example&lt;/a&gt; for a classification problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Boost for Regression - Example</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_regression_example/</link>
      <pubDate>Tue, 09 Apr 2024 22:55:13 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_regression_example/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In this post, we will go through the development of a Gradient Boosting model for a regression problem, considering a simplified example. We calculate the individual steps in detail, which are defined and explained in the separate post &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_regression/&#34;&gt;Gradient Boost for Regression - Explained&lt;/a&gt;. Please refer to this post for a more general and detailed explanation of the algorithm.&lt;/p&gt;&#xA;&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;&#xA;&lt;p&gt;We will use a simplified dataset consisting of only 10 samples, which describes how many meters a person has climbed, depending on their age, whether they like height, and whether they like goats. We used that same data in previous posts, such as &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_tree_regression_example/&#34;&gt;Decision Trees for Regression - Example&lt;/a&gt;, and &lt;a href=&#34;http://localhost:1313/posts/classical_ml/adaboost_example_reg/&#34;&gt;Adaboost for Regression - Example&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/backpropagation/</link>
      <pubDate>Sun, 31 Mar 2024 20:21:50 -0300</pubDate>
      <guid>http://localhost:1313/backpropagation/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A neural network consists of a set of parameters - the weights and biases - which define the outcome of the network, that is the predictions. When training a neural network we aim to adjust these weights and biases such that the predictions improve. To achieve that &lt;em&gt;Backpropagation&lt;/em&gt; is used. In this post, we discuss how backpropagation works, and explain it in detail for four simple examples. The first two examples will contain all the calculations, the last two will only illustrate the equations that need to be calculated. Additionally, the general formulation is shown, but without going into details.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Backpropagation Step by Step</title>
      <link>http://localhost:1313/posts/deep_learning/backpropagation/</link>
      <pubDate>Sun, 31 Mar 2024 20:21:50 -0300</pubDate>
      <guid>http://localhost:1313/posts/deep_learning/backpropagation/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A neural network consists of a set of parameters - the weights and biases - which define the outcome of the network, that is the predictions. When training a neural network we aim to adjust these weights and biases such that the predictions improve. To achieve that &lt;em&gt;Backpropagation&lt;/em&gt; is used. In this post, we discuss how backpropagation works, and explain it in detail for three simple examples. The first two examples will contain all the calculations, for the last one we will only illustrate the equations that need to be calculated. We will not go into the general formulation of the backpropagation algorithm but will give some further readings at the end.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Descent</title>
      <link>http://localhost:1313/posts/ml_concepts/gradient_descent/</link>
      <pubDate>Tue, 27 Feb 2024 20:55:28 -0300</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/gradient_descent/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Gradient Descent is a mathematical optimization technique, which is used to find the local minima of a function. In Machine Learning it is used in a variety of models such as &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_regression/&#34; title=&#34;gradient_boosting&#34;&gt;Gradient Boosting&lt;/a&gt; or &lt;a href=&#34;http://localhost:1313/posts/deep_learning/intro_dl/&#34;&gt;Neural Networks&lt;/a&gt; to minimize the &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/loss_functions/&#34;&gt;Loss Function&lt;/a&gt;. It is an iterative algorithm that takes small steps towards the minimum in every iteration. The idea is to start at a random point and then take a small step into the direction of the steepest descent of this point. Then take another small step in the direction of the steepest descent of the next point and so on. The direction of the steepest descent is determined using the gradient of the function, so its name &lt;em&gt;Gradient Descent&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Loss Functions in Machine Learning</title>
      <link>http://localhost:1313/posts/ml_concepts/loss_functions/</link>
      <pubDate>Sun, 04 Feb 2024 18:57:51 -0300</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/loss_functions/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In Machine Learning loss functions are used to evaluate the model. They compare the true target values with the predicted ones and are directly related to the error of the predictions. During the training of a model, the loss function is aimed to be optimized to minimize the error of the predictions. It is a general convention to define a loss function such that it is minimized rather than maximized. The specific choice of a loss function depends on the problem we want to solve, e.g. whether a regression or a classification task is considered. In this article, we will discuss the most common ones, which work very well for a lot of tasks. We can, however, also create custom loss functions adapted for specific problems. Custom loss functions help to focus on the specific errors we aim to minimize. We will look at examples of custom loss functions later in this post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gradient Boost for Regression - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_regression/</link>
      <pubDate>Wed, 31 Jan 2024 09:21:46 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_regression/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Gradient Boosting&lt;/em&gt;, also called &lt;em&gt;Gradient Boosting Machine (GBM)&lt;/em&gt; is a type of &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/supervised_unsupervised/#supervised&#34;&gt;supervised&lt;/a&gt; Machine Learning algorithm that is based on &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/ensemble/&#34;&gt;ensemble learning&lt;/a&gt;. It consists of a sequential series of models, each one trying to improve the errors of the previous one. It can be used for both regression and classification tasks. In this post, we introduce the algorithm and then explain it in detail for a regression task. We will look at the general formulation of the algorithm and then derive and simplify the individual steps for the most common use case, which uses Decision Trees as underlying models and a variation of the &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/regression_metrics/#metrics&#34;&gt;Mean Squared Error (MSE)&lt;/a&gt; as loss function. Please find a detailed example, where this is applied to a specific dataset in the separate article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_regression_example/&#34;&gt;Gradient Boosting for Regression - Example&lt;/a&gt;. Gradient Boosting can also be applied for classification tasks. This is covered in the articles &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_classification/&#34;&gt;Gradient Boosting for Classification - Explained&lt;/a&gt; and &lt;a href=&#34;http://localhost:1313/posts/classical_ml/gradient_boosting_classification_example/&#34;&gt;Gradient Boosting for Classification - Example&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adaboost for Regression - Example</title>
      <link>http://localhost:1313/posts/classical_ml/adaboost_example_reg/</link>
      <pubDate>Fri, 19 Jan 2024 23:05:44 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/adaboost_example_reg/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;AdaBoost is an ensemble model that sequentially builds new models based on the errors of the previous model to improve the predictions. The most common case is to use Decision Trees as base models. Very often the examples explained are for classification tasks. AdaBoost can, however, also be used for regression problems. This is what we will focus on in this post. This article covers the detailed calculations of a simplified example. For a general explanation of the algorithm, please refer to &lt;a href=&#34;http://localhost:1313/posts/classical_ml/adaboost/&#34;&gt;AdaBoost - Explained&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaBoost for Classification - Example</title>
      <link>http://localhost:1313/posts/classical_ml/adaboost_example_clf/</link>
      <pubDate>Wed, 17 Jan 2024 22:08:14 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/adaboost_example_clf/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;AdaBoost is an &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/ensemble/&#34;&gt;ensemble&lt;/a&gt; model that is based on &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/ensemble/#boosting&#34;&gt;Boosting&lt;/a&gt;. The individual models are so-called weak learners, which means that they have only little predictive skill, and they are sequentially built to improve the errors of the previous one. A detailed description of the Algorithm can be found in the separate article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/adaboost/&#34;&gt;AdaBoost - Explained&lt;/a&gt;. In this post, we will focus on a concrete example for a classification task and develop the final ensemble model in detail. A detailed example of a regression task is given in the article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/adaboost_example_reg/&#34;&gt;AdaBoost for Regression - Example&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaBoost - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/adaboost/</link>
      <pubDate>Sun, 14 Jan 2024 09:22:00 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/adaboost/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;AdaBoost is an example of an &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/ensemble/&#34;&gt;ensemble&lt;/a&gt; &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/supervised_unsupervised/#supervised&#34;&gt;supervised&lt;/a&gt; Machine Learning model. It consists of a sequential series of models, each one focussing on the errors of the previous one, trying to improve them. The most common underlying model is the &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_trees/&#34;&gt;Decision Tree&lt;/a&gt;, other models are however possible. In this post, we will introduce the algorithm of AdaBoost and have a look at a simplified example for a classification task using &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html&#34;&gt;sklearn&lt;/a&gt;. For a more detailed exploration of this example - deriving it by hand - please refer to &lt;a href=&#34;http://localhost:1313/posts/classical_ml/adaboost_example_clf/&#34;&gt;AdaBoost for Classification - Example&lt;/a&gt;. A more realistic example with a larger dataset is provided on &lt;a href=&#34;https://www.kaggle.com/pumalin/adaboost-tutorial&#34;&gt;kaggle&lt;/a&gt;. Accordingly, if you are interested in how AdaBoost is developed for a regression task, please check the article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/adaboost_example_reg/&#34;&gt;AdaBoost for Regression - Example&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bias and Variance</title>
      <link>http://localhost:1313/posts/ml_concepts/bias_variance/</link>
      <pubDate>Mon, 01 Jan 2024 09:39:26 +0100</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/bias_variance/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In Machine Learning different error sources exist. Some errors cannot be avoided, for example, due to unknown variables in the system analyzed. These errors are called &lt;em&gt;irreducible errors&lt;/em&gt;. On the other hand, &lt;em&gt;reducible errors&lt;/em&gt;, are errors that can be reduced to improve the model&amp;rsquo;s skill. &lt;em&gt;Bias&lt;/em&gt; and &lt;em&gt;Variance&lt;/em&gt; are two of the latter. They are concepts used in supervised Machine Learning to evaluate the model&amp;rsquo;s output compared to the true values. For a Machine Learning model to be generalizable to new unseen data with high predictive skill, it is important that bias and variance are balanced.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ensemble Models - Illustrated</title>
      <link>http://localhost:1313/posts/ml_concepts/ensemble/</link>
      <pubDate>Tue, 26 Dec 2023 11:24:29 +0100</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/ensemble/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In &lt;em&gt;Ensemble Learning&lt;/em&gt; multiple Machine Learning models are combined into one single prediction to improve the predictive skill. The individual models can be of different types or the same. Ensemble learning is based on &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds&#34;&gt;&amp;ldquo;the wisdom of the crowds&amp;rdquo;&lt;/a&gt;, which assumes that the expected value of multiple estimates is more accurate than a single estimate. Ensemble learning can be used for regression or classification tasks. Three main types of Ensemble Learning method are most common.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Random Forests - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/random_forest/</link>
      <pubDate>Tue, 26 Dec 2023 10:57:13 +0100</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/random_forest/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A Random Forest is a &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/supervised_unsupervised/#supervised&#34;&gt;supervised&lt;/a&gt; Machine Learning model, that is built on Decision Trees. To understand how a Random Forest works, you should be familiar with Decision Trees. You can find an introduction in the separate article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_trees/&#34;&gt;Decision Trees - Explained&lt;/a&gt;. A major disadvantage of Decision Trees is that they tend to overfit and often have difficulties to generalize to new data. Random Forests try to overcome this weakness. They are built of a set of Decision Trees, which are combined into an ensemble model, and their outcomes are converted into a single result. As Decision Trees, they can be used for classification and regression tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decision Trees for Regression - Example</title>
      <link>http://localhost:1313/posts/classical_ml/decision_tree_regression_example/</link>
      <pubDate>Tue, 19 Dec 2023 17:46:29 +0100</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/decision_tree_regression_example/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A Decision Tree is a simple Machine Learning model that can be used for both regression and classification tasks. In the article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_tree_classification_example/&#34;&gt;Decision Trees for Classification - Example&lt;/a&gt; a Decision Tree for a classification problem is developed in detail. In this post, we consider a regression problem and build a Decision Tree step by step for a simplified dataset. Additionally, we use &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html&#34;&gt;sklearn&lt;/a&gt; to fit a model to the data and compare the results. To learn about Decision Trees in a more general setup, please refer to &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_trees/&#34;&gt;Decision Trees - Explained&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decision Trees for Classification - Example</title>
      <link>http://localhost:1313/posts/classical_ml/decision_tree_classification_example/</link>
      <pubDate>Tue, 19 Dec 2023 09:11:46 +0100</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/decision_tree_classification_example/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Decision Trees are a powerful, yet simple Machine Learning Model. An advantage of their simplicity is that we can build and understand them step by step. In this post, we are looking at a simplified example to build an entire Decision Tree by hand for a classification task. After calculating the tree, we will use the &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html&#34;&gt;sklearn&lt;/a&gt; package and compare the results. To learn how to build a Decision Tree for a regression problem, please refer to the article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_tree_regression_example/&#34;&gt;Decision Trees for Regression - Example&lt;/a&gt;. For a general introduction to Decision Trees and how to build them please check the article &lt;a href=&#34;http://localhost:1313/posts/classical_ml/decision_trees/&#34;&gt;Decision Trees - Explained&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decision Trees - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/decision_trees/</link>
      <pubDate>Sat, 16 Dec 2023 12:33:55 +0100</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/decision_trees/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A &lt;em&gt;Decision Tree&lt;/em&gt; is a &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/supervised_unsupervised/#supervised&#34;&gt;supervised Machine Learning&lt;/a&gt; algorithm that can be used for both regression and classification problems. It is a non-parametric model, which means there is no specific mathematical function underlying to fit the data (in contrast to e.g. &lt;a href=&#34;http://localhost:1313/posts/classical_ml/linear_regression/&#34;&gt;Linear Regression&lt;/a&gt; or &lt;a href=&#34;http://localhost:1313/posts/classical_ml/logistic_regression/&#34;&gt;Logistic Regression&lt;/a&gt;), but the algorithm only learns from the data itself. Decision Trees learn rules for decision making and used to be drawn manually before Machine Learning came up. They are hierarchical models, that have a flow-chart tree structure as the name suggests.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Feature Selection Methods</title>
      <link>http://localhost:1313/posts/ml_concepts/feature_selection/</link>
      <pubDate>Mon, 11 Dec 2023 22:56:54 +0100</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/feature_selection/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Feature Selection&lt;/em&gt; is the process of determining the most suitable subset of the total number of available features for modeling. It helps to understand which features contribute most to the target data. This is usefull to&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Improve Model Performance.&lt;/strong&gt; Redundant and irrelevant features may be misleading for the model. Additionally, if the feature space is too large compared to the sample size. This is called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Curse_of_dimensionality&#34;&gt;curse of dimensionality&lt;/a&gt; and may reduce the model&amp;rsquo;s performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logistic Regression - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/logistic_regression/</link>
      <pubDate>Sat, 02 Dec 2023 09:31:24 +0100</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/logistic_regression/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Logistic Regression is a &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/supervised_unsupervised/#supervised&#34; title=&#34;Supervised Machine Learning&#34;&gt;Supervised Machine Learning&lt;/a&gt; algorithm, in which a model is developed, that relates the target variable to one or more input variables (features). However, in contrast to &lt;a href=&#34;http://localhost:1313/posts/classical_ml/linear_regression/&#34; title=&#34;Linear Regression&#34;&gt;Linear Regression&lt;/a&gt; the target (dependent) variable is not numerical, but &lt;a href=&#34;https://en.wikipedia.org/wiki/Categorical_variable&#34;&gt;categorical&lt;/a&gt;. That is the target variable can be classified in different categories (e.g.: &amp;rsquo;test passed&amp;rsquo; or &amp;rsquo;test not passed&amp;rsquo;). An idealized example of two categories for the target variable is illustrated in the plot below. The relation described in this example is whether a test is passed or not, depending on the amount of hours studied. Note, that in real world examples the border between the two classes depending on the input feature (independ variable) will usually not be as clear as in this plot.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Regression - Analytical Solution and Simplified Example</title>
      <link>http://localhost:1313/posts/classical_ml/linear_regression_example/</link>
      <pubDate>Thu, 23 Nov 2023 11:00:02 +0100</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/linear_regression_example/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In a previous article, we introduced &lt;a href=&#34;http://localhost:1313/posts/classical_ml/linear_regression/&#34; title=&#34;Linear Regression&#34;&gt;Linear Regression&lt;/a&gt; in detail and more generally, showed how to find the best model and discussed its chances and limitations. In this post, we are looking at a concrete example. We are going to calculate the &lt;em&gt;slope&lt;/em&gt; and the &lt;em&gt;intercept&lt;/em&gt; from a Simple Linear Regression analytically, looking at the example data provided in the next plot.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/20231001_regression_metrics/regression_example.jpg&#34; alt=&#34;regression example&#34;&gt;&#xA;&lt;em&gt;Illustration of a simple linear regression between the body mass and the maximal running speed of an animal.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Regression - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/linear_regression/</link>
      <pubDate>Mon, 13 Nov 2023 21:20:41 +0100</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/linear_regression/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Linear Regression is a type of &lt;a href=&#34;http://localhost:1313/posts/ml_concepts/supervised_unsupervised/#supervised&#34; title=&#34;Supervised Machine Learning&#34;&gt;Supervised Machine Learning&lt;/a&gt; Algorithm, where a linear relationship between the input feature(s) and the target value is assumed. Linear Regression is a specific type of regression model, where the mapping learned by the model describes a linear function. As in all regression tasks, the target variable is continuous. In a linear regression, the linear relationship between one (&lt;a href=&#34;#slr&#34;&gt;Simple Linear Regression&lt;/a&gt;) or more (&lt;a href=&#34;#mrl&#34;&gt;Multiple Linear Regression&lt;/a&gt;) independent variable and one dependent variable is modeled.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Deep Learning</title>
      <link>http://localhost:1313/posts/deep_learning/intro_dl/</link>
      <pubDate>Thu, 02 Nov 2023 21:44:06 +0100</pubDate>
      <guid>http://localhost:1313/posts/deep_learning/intro_dl/</guid>
      <description>&lt;p&gt;In this article we will learn what Deep Learning is and understand the difference to AI and Machine Learning. Often these three terms are used interchangeable. They are however not the same. The following diagram illustrates how they are related.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/20231102_ai_ml_dl/ai_ml_dl.png&#34; alt=&#34;ai_ml_dl&#34;&gt;&#xA;&lt;em&gt;Relation of Artificial Intelligence, Machine Learning and Deep Learning.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Artificial Intelligence.&lt;/strong&gt; There are different definitions of Artificial Intelligence, but in general, they involve computers performing tasks that are usually associated with humans or other intelligent living systems. This is the definition given on Wikimedia:&#xA;It says: “[AI is] Mimicking the intelligence or behavioral pattern of humans or any living entity”&lt;/p&gt;</description>
    </item>
    <item>
      <title>Supervised versus Unsupervised Learning - Explained</title>
      <link>http://localhost:1313/posts/ml_concepts/supervised_unsupervised/</link>
      <pubDate>Tue, 17 Oct 2023 09:46:31 +0200</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/supervised_unsupervised/</guid>
      <description>&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;&#xA;&lt;p&gt;In classical programming, the programmer defines specific rules which the program follows and these rules lead to an output. In contrast, Machine Learning uses data to find the rules that describe the relationship between input and output. This process of finding the rules is called &amp;rsquo;learning&amp;rsquo;. Supervised and Unsupervised Learning are two different types of Machine Learning. Let&amp;rsquo;s discover what each means.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/20231017_supervised_unsupervised/supervised_unsupervised.gif&#34; alt=&#34;supervised_unsupervised&#34;&gt;&#xA;&lt;em&gt;Fig. 1: Supervised and Unsupervised Learning are different types of Machine Learning.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Metrics for Classification Problems</title>
      <link>http://localhost:1313/posts/ml_concepts/classification_metrics/</link>
      <pubDate>Sun, 01 Oct 2023 00:08:49 +0200</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/classification_metrics/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/20230920_classification_metrics/classification_metrics.gif&#34; alt=&#34;classification metrics&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;classification-problems&#34;&gt;Classification Problems&lt;/h2&gt;&#xA;&lt;p&gt;Supervised Machine Learning projects can be divided into &lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_analysis&#34;&gt;regression&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_classification&#34;&gt;classification&lt;/a&gt; problems. In regression problems, we predict a continuous variable (e.g. temperature), while in classification, we classify the data into discrete classes (e.g. classify cat and dog images). A subset of classification problems is the so-called &lt;a href=&#34;https://en.wikipedia.org/wiki/Binary_classification&#34;&gt;binary classification&lt;/a&gt;, where only two classes are considered. An example of this is classifying e-mails as spam and no-spam or cat images versus dog images. When we consider more than two classes, we speak of &lt;a href=&#34;https://en.wikipedia.org/wiki/Multiclass_classification&#34;&gt;multi-class classification&lt;/a&gt;. An example of this is the &lt;a href=&#34;https://en.wikipedia.org/wiki/MNIST_database&#34;&gt;MNIST&lt;/a&gt; dataset, where images of handwritten digits are classified, i.e. we have 10 different classes. Depending on the problem we consider, we need different metrics to evaluate our Machine Learning model. Within each of these two types of Machine Learning problems, we have to choose which of the metrics fits our needs best. This post will give an overview of the most common metrics for &lt;strong&gt;binary classification&lt;/strong&gt; problems, what they mean, and when to use them.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Metrics for Regression Problems</title>
      <link>http://localhost:1313/posts/ml_concepts/regression_metrics/</link>
      <pubDate>Sat, 30 Sep 2023 21:24:12 +0200</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/regression_metrics/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/20231001_regression_metrics/regression_metrics.jpg&#34; alt=&#34;regression metrics&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;regression-problems&#34;&gt;Regression Problems&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_analysis&#34;&gt;Regression&lt;/a&gt; problems in Machine Learning are a type of supervised learning problem, where a continuous numerical variable is predicted, such as, for example, the age of a person or the price of a product. A special type is the &lt;a href=&#34;http://localhost:1313/posts/classical_ml/linear_regression/&#34; title=&#34;Linear Regression&#34;&gt;Linear Regression&lt;/a&gt;, where a linear relationship between two (&lt;a href=&#34;http://localhost:1313/posts/classical_ml/linear_regression/#slr&#34; title=&#34;Linear Regression&#34;&gt;Simple Linear Regression&lt;/a&gt; or more (&lt;a href=&#34;http://localhost:1313/posts/classical_ml/linear_regression/#mlr&#34; title=&#34;Linear Regression&#34;&gt;Multiple Linear Regression&lt;/a&gt;) is analyzed. The example plots in this article will be illustrated with a simple linear regression. However the metrics introduced here are common metrics for all types of regression problems, including multiple linear regression and non-linear regression. The simple linear regression is only chosen for illustration purposes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Data Science Lifecycle</title>
      <link>http://localhost:1313/posts/ml_concepts/datascience_lifecycle/</link>
      <pubDate>Wed, 13 Sep 2023 21:28:42 +0200</pubDate>
      <guid>http://localhost:1313/posts/ml_concepts/datascience_lifecycle/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;When we think about Data Science, we usually think about Machine Learning modeling. However, a Data Science project consists of many more steps. Whereas modelling might be the most fun part, it is important to know that this is only a fraction of the entire lifecycle of a Data Science project. When we plan a project and communicate how much time we need, we need to make sure that enough time is given for all the surrounding tasks. Let&amp;rsquo;s have a look at the individual steps of a Data Science project.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
