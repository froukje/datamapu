<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Boosting on </title>
    <link>http://localhost:1313/tags/boosting/</link>
    <description>Recent content in Boosting on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jan 2024 09:21:46 -0300</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/boosting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gradient Boost for Regression - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_regression/</link>
      <pubDate>Wed, 31 Jan 2024 09:21:46 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_regression/</guid>
      <description>Introduction Gradient Boosting, also called Gradient Boosting Machine (GBM) is a type of supervised Machine Learning algorithm that is based on ensemble learning. It consists of a sequential series of models, each one trying to improve the errors of the previous one. It can be used for both regression and classification tasks. In this post we introduce the algorithm and then explain it in detail for a regression task. We will have a look at the general formulation of the algorithm and then derive and simplify the individual steps for the most common use case, which uses Decision Trees as underlying models and a variation of the Mean Squared Error (MSE) as loss function.</description>
    </item>
    <item>
      <title>Adaboost for Regression - Example</title>
      <link>http://localhost:1313/posts/classical_ml/adaboost_example_reg/</link>
      <pubDate>Fri, 19 Jan 2024 23:05:44 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/adaboost_example_reg/</guid>
      <description>Introduction AdaBoost is an ensemble model that sequentially builds new models based on the errors of the previous model to improve the predictions. The most common case is to use Decision Trees as base models. Very often the examples explained are for classification tasks. AdaBoost can, however, also be used for regression problems. This is what we will focus on in this post. This article covers the detailed calculations of a simplified example.</description>
    </item>
    <item>
      <title>AdaBoost for Classification - Example</title>
      <link>http://localhost:1313/posts/classical_ml/adaboost_example_clf/</link>
      <pubDate>Wed, 17 Jan 2024 22:08:14 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/adaboost_example_clf/</guid>
      <description>Introduction AdaBoost is an ensemble model that is based on Boosting. The individual models are so-called weak learners, which means that they have only little predictive skill, and they are sequentially built to improve the errors of the previous one. A detailed description of the Algorithm can be found in the separate article AdaBoost - Explained. In this post, we will focus on a concrete example for a classification task and develop the final ensemble model in detail.</description>
    </item>
    <item>
      <title>AdaBoost - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/adaboost/</link>
      <pubDate>Sun, 14 Jan 2024 09:22:00 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/adaboost/</guid>
      <description>Introduction AdaBoost is an example of an ensemble supervised Machine Learning model. It consists of a sequential series of models, each one focussing on the errors of the previous one, trying to improve them. The most common underlying model is the Decision Tree, other models are however possible. In this post, we will introduce the algorithm of AdaBoost and have a look at a simplified example for a classification task using sklearn.</description>
    </item>
  </channel>
</rss>
