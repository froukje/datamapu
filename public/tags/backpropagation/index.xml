<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Backpropagation on </title>
    <link>http://localhost:1313/tags/backpropagation/</link>
    <description>Recent content in Backpropagation on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Mar 2024 20:21:50 -0300</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/backpropagation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Backpropagation Step by Step</title>
      <link>http://localhost:1313/posts/deep_learning/backpropagation/</link>
      <pubDate>Sun, 31 Mar 2024 20:21:50 -0300</pubDate>
      <guid>http://localhost:1313/posts/deep_learning/backpropagation/</guid>
      <description>Introduction A neural network consists of a set of parameters - the weights and biases - that define the outcome of the network, that is the predictions. When training a neural network we aim to adjust these weights and biases such that the predictions improve. In order to achieve that Backpropagation is used. In this post we discuss how backpropagation works, and explain it in detail for different examples. We additionally show the general formulation, but without going into details.</description>
    </item>
  </channel>
</rss>
