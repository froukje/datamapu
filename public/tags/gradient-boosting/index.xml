<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gradient Boosting on </title>
    <link>http://localhost:1313/tags/gradient-boosting/</link>
    <description>Recent content in Gradient Boosting on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 22:55:13 -0300</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/gradient-boosting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gradient Boost for Regression - Example</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_regression_example/</link>
      <pubDate>Tue, 09 Apr 2024 22:55:13 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_regression_example/</guid>
      <description>Introduction In this post we will go through the development of a Gradient Boosting model for a regression problem considering a simplified example. We will calculate the individual steps defined and explained in the seperate post Gradient Boost for Regression - Explained. Please refer to this post for a more general and detailed explanation of the algorithm.&#xA;Data We will use a simplified dataset consisting of only 10 samples, which describes how many meters a person has climbed, depending on their age, whether or not they like height, and whether or not they like goats.</description>
    </item>
    <item>
      <title>Gradient Boost for Regression - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_regression/</link>
      <pubDate>Wed, 31 Jan 2024 09:21:46 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_regression/</guid>
      <description>Introduction Gradient Boosting, also called Gradient Boosting Machine (GBM) is a type of supervised Machine Learning algorithm that is based on ensemble learning. It consists of a sequential series of models, each one trying to improve the errors of the previous one. It can be used for both regression and classification tasks. In this post we introduce the algorithm and then explain it in detail for a regression task. We will have a look at the general formulation of the algorithm and then derive and simplify the individual steps for the most common use case, which uses Decision Trees as underlying models and a variation of the Mean Squared Error (MSE) as loss function.</description>
    </item>
  </channel>
</rss>
