<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Classification on </title>
    <link>http://localhost:46267/categories/classification/</link>
    <description>Recent content in Classification on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Apr 2024 17:01:32 -0300</lastBuildDate>
    <atom:link href="http://localhost:46267/categories/classification/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gradient Boost for Classification Example</title>
      <link>http://localhost:46267/posts/classical_ml/gradient_boosting_classification_example/</link>
      <pubDate>Sun, 28 Apr 2024 17:01:32 -0300</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/gradient_boosting_classification_example/</guid>
      <description>Introduction In this post, we develop a Gradient Boosting model for a binary classification. We focus on the calculations of each single step for a specific example chosen. For a more general explanation of the algorithm and the derivation of the formulas for the individual steps, please refer to Gradient Boost for Classification - Explained and Gradient Boost for Regression - Explained. Additionally, we show a simple example of how to apply Gradient Boosting for classification in Python.</description>
    </item>
    <item>
      <title>Gradient Boost for Classification - Explained</title>
      <link>http://localhost:46267/posts/classical_ml/gradient_boosting_classification/</link>
      <pubDate>Sun, 14 Apr 2024 20:45:19 -0300</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/gradient_boosting_classification/</guid>
      <description>Introduction Gradient Boosting is an ensemble machine learning model, that - as the name suggests - is based on boosting. An ensemble model based on boosting refers to a model that sequentially builds models, and the new model depends on the previous model. In Gradient Boosting these models are built such that they improve the error of the previous model. These individual models are so-called weak learners, which means they have low predictive skills.</description>
    </item>
    <item>
      <title>Adaboost for Regression - Example</title>
      <link>http://localhost:46267/posts/classical_ml/adaboost_example_reg/</link>
      <pubDate>Fri, 19 Jan 2024 23:05:44 -0300</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/adaboost_example_reg/</guid>
      <description>Introduction AdaBoost is an ensemble model that sequentially builds new models based on the errors of the previous model to improve the predictions. The most common case is to use Decision Trees as base models. Very often the examples explained are for classification tasks. AdaBoost can, however, also be used for regression problems. This is what we will focus on in this post. This article covers the detailed calculations of a simplified example.</description>
    </item>
    <item>
      <title>AdaBoost for Classification - Example</title>
      <link>http://localhost:46267/posts/classical_ml/adaboost_example_clf/</link>
      <pubDate>Wed, 17 Jan 2024 22:08:14 -0300</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/adaboost_example_clf/</guid>
      <description>Introduction AdaBoost is an ensemble model that is based on Boosting. The individual models are so-called weak learners, which means that they have only little predictive skill, and they are sequentially built to improve the errors of the previous one. A detailed description of the Algorithm can be found in the separate article AdaBoost - Explained. In this post, we will focus on a concrete example for a classification task and develop the final ensemble model in detail.</description>
    </item>
    <item>
      <title>AdaBoost - Explained</title>
      <link>http://localhost:46267/posts/classical_ml/adaboost/</link>
      <pubDate>Sun, 14 Jan 2024 09:22:00 -0300</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/adaboost/</guid>
      <description>Introduction AdaBoost is an example of an ensemble supervised Machine Learning model. It consists of a sequential series of models, each one focussing on the errors of the previous one, trying to improve them. The most common underlying model is the Decision Tree, other models are however possible. In this post, we will introduce the algorithm of AdaBoost and have a look at a simplified example for a classification task using sklearn.</description>
    </item>
    <item>
      <title>Ensemble Models - Illustrated</title>
      <link>http://localhost:46267/posts/ml_concepts/ensemble/</link>
      <pubDate>Tue, 26 Dec 2023 11:24:29 +0100</pubDate>
      <guid>http://localhost:46267/posts/ml_concepts/ensemble/</guid>
      <description>Introduction In Ensemble Learning multiple Machine Learning models are combined into one single prediction to improve the predictive skill. The individual models can be of different types or the same. Ensemble learning is based on &amp;ldquo;the wisdom of the crowds&amp;rdquo;, which assumes that the expected value of multiple estimates is more accurate than a single estimate. Ensemble learning can be used for regression or classification tasks. Three main types of Ensemble Learning method are most common.</description>
    </item>
    <item>
      <title>Random Forests - Explained</title>
      <link>http://localhost:46267/posts/classical_ml/random_forest/</link>
      <pubDate>Tue, 26 Dec 2023 10:57:13 +0100</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/random_forest/</guid>
      <description>Introduction A Random Forest is a supervised Machine Learning model, that is built on Decision Trees. To understand how a Random Forest works, you should be familiar with Decision Trees. You can find an introduction in the separate article Decision Trees - Explained. A major disadvantage of Decision Trees is that they tend to overfit and often have difficulties to generalize to new data. Random Forests try to overcome this weakness.</description>
    </item>
    <item>
      <title>Decision Trees for Classification - Example</title>
      <link>http://localhost:46267/posts/classical_ml/decision_tree_classification_example/</link>
      <pubDate>Tue, 19 Dec 2023 09:11:46 +0100</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/decision_tree_classification_example/</guid>
      <description>Introduction Decision Trees are a powerful, yet simple Machine Learning Model. An advantage of their simplicity is that we can build and understand them step by step. In this post, we are looking at a simplified example to build an entire Decision Tree by hand for a classification task. After calculating the tree, we will use the sklearn package and compare the results. To learn how to build a Decision Tree for a regression problem, please refer to the article Decision Trees for Regression - Example.</description>
    </item>
    <item>
      <title>Decision Trees - Explained</title>
      <link>http://localhost:46267/posts/classical_ml/decision_trees/</link>
      <pubDate>Sat, 16 Dec 2023 12:33:55 +0100</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/decision_trees/</guid>
      <description>Introduction A Decision Tree is a supervised Machine Learning algorithm that can be used for both regression and classification problems. It is a non-parametric model, which means there is no specific mathematical function underlying to fit the data (in contrast to e.g. Linear Regression or Logistic Regression), but the algorithm only learns from the data itself. Decision Trees learn rules for decision making and used to be drawn manually before Machine Learning came up.</description>
    </item>
    <item>
      <title>Feature Selection Methods</title>
      <link>http://localhost:46267/posts/ml_concepts/feature_selection/</link>
      <pubDate>Mon, 11 Dec 2023 22:56:54 +0100</pubDate>
      <guid>http://localhost:46267/posts/ml_concepts/feature_selection/</guid>
      <description>Introduction Feature Selection is the process of determining the most suitable subset of the total number of available features for modeling. It helps to understand which features contribute most to the target data. This is usefull to&#xA;Improve Model Performance. Redundant and irrelevant features may be misleading for the model. Additionally, if the feature space is too large compared to the sample size. This is called the curse of dimensionality and may reduce the model&amp;rsquo;s performance.</description>
    </item>
    <item>
      <title>Logistic Regression - Explained</title>
      <link>http://localhost:46267/posts/classical_ml/logistic_regression/</link>
      <pubDate>Sat, 02 Dec 2023 09:31:24 +0100</pubDate>
      <guid>http://localhost:46267/posts/classical_ml/logistic_regression/</guid>
      <description>Introduction Logistic Regression is a Supervised Machine Learning algorithm, in which a model is developed, that relates the target variable to one or more input variables (features). However, in contrast to Linear Regression the target (dependent) variable is not numerical, but categorical. That is the target variable can be classified in different categories (e.g.: &amp;rsquo;test passed&amp;rsquo; or &amp;rsquo;test not passed&amp;rsquo;). An idealized example of two categories for the target variable is illustrated in the plot below.</description>
    </item>
  </channel>
</rss>
