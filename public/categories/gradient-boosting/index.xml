<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gradient Boosting on </title>
    <link>http://localhost:1313/categories/gradient-boosting/</link>
    <description>Recent content in Gradient Boosting on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jan 2024 09:21:46 -0300</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/gradient-boosting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gradient Boost for Regression - Explained</title>
      <link>http://localhost:1313/posts/classical_ml/gradient_boosting_regression/</link>
      <pubDate>Wed, 31 Jan 2024 09:21:46 -0300</pubDate>
      <guid>http://localhost:1313/posts/classical_ml/gradient_boosting_regression/</guid>
      <description>Introduction Gradient Boosting, also called Gradient Boosting Machine (GBM) is a type of supervised Machine Learning algorithm that is based on ensemble learning. It consists of a sequential series of models, each one trying to improve the errors of the previous one. It can be used for both regression and classification tasks. In this post we introduce the algorithm and then explain it in detail for a regression task. We will have a look at the general formulation of the algorithm and then derive and simplify the individual steps for the most common use case, which uses Decision Trees as underlying models and a variation of the Mean Squared Error (MSE) as loss function.</description>
    </item>
  </channel>
</rss>
