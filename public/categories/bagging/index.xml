<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bagging on </title>
    <link>http://localhost:32875/categories/bagging/</link>
    <description>Recent content in Bagging on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Dec 2023 10:57:13 +0100</lastBuildDate>
    <atom:link href="http://localhost:32875/categories/bagging/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Random Forests - Explained</title>
      <link>http://localhost:32875/posts/classical_ml/random_forest/</link>
      <pubDate>Tue, 26 Dec 2023 10:57:13 +0100</pubDate>
      <guid>http://localhost:32875/posts/classical_ml/random_forest/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A Random Forest is a &lt;a href=&#34;http://localhost:32875/posts/ml_concepts/supervised_unsupervised/#supervised&#34;&gt;supervised&lt;/a&gt; Machine Learning model, that is built on Decision Trees. To understand how a Random Forest works, you should be familiar with Decision Trees. You can find an introduction in the separate article &lt;a href=&#34;http://localhost:32875/posts/classical_ml/decision_trees/&#34;&gt;Decision Trees - Explained&lt;/a&gt;. A major disadvantage of Decision Trees is that they tend to overfit and often have difficulties to generalize to new data. Random Forests try to overcome this weakness. They are built of a set of Decision Trees, which are combined into an ensemble model, and their outcomes are converted into a single result. As Decision Trees, they can be used for classification and regression tasks.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
