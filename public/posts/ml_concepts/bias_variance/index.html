<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Bias and Variance | </title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Introduction In Machine Learning different error sources exist. Some errors cannot be avoided, for example, due to unknown variables in the system analyzed. These errors are called irreducible errors. On the other hand, reducible errors, are errors that can be reduced to improve the model&rsquo;s skill. Bias and Variance are two of the latter. They are concepts used in supervised Machine Learning to evaluate the model&rsquo;s output compared to the true values.">
    <meta name="generator" content="Hugo 0.125.3">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />


    

    
    
    <meta property="og:url" content="http://localhost:1313/posts/ml_concepts/bias_variance/">
  <meta property="og:title" content="Bias and Variance">
  <meta property="og:description" content="Introduction In Machine Learning different error sources exist. Some errors cannot be avoided, for example, due to unknown variables in the system analyzed. These errors are called irreducible errors. On the other hand, reducible errors, are errors that can be reduced to improve the model&amp;rsquo;s skill. Bias and Variance are two of the latter. They are concepts used in supervised Machine Learning to evaluate the model&amp;rsquo;s output compared to the true values.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-01T09:39:26+01:00">
    <meta property="article:modified_time" content="2024-01-01T09:39:26+01:00">
    <meta property="article:tag" content="Data Science">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Artificial Intelligence">
    <meta property="og:image" content="http://localhost:1313/images/bias_variance/bias_variance_4.png">

  <meta itemprop="name" content="Bias and Variance">
  <meta itemprop="description" content="Introduction In Machine Learning different error sources exist. Some errors cannot be avoided, for example, due to unknown variables in the system analyzed. These errors are called irreducible errors. On the other hand, reducible errors, are errors that can be reduced to improve the model&rsquo;s skill. Bias and Variance are two of the latter. They are concepts used in supervised Machine Learning to evaluate the model&rsquo;s output compared to the true values.">
  <meta itemprop="datePublished" content="2024-01-01T09:39:26+01:00">
  <meta itemprop="dateModified" content="2024-01-01T09:39:26+01:00">
  <meta itemprop="wordCount" content="1676">
  <meta itemprop="image" content="http://localhost:1313/images/bias_variance/bias_variance_4.png">
  <meta itemprop="keywords" content="Data Science,Machine Learning,Artificial Intelligence"><meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/images/bias_variance/bias_variance_4.png"><meta name="twitter:title" content="Bias and Variance">
<meta name="twitter:description" content="Introduction In Machine Learning different error sources exist. Some errors cannot be avoided, for example, due to unknown variables in the system analyzed. These errors are called irreducible errors. On the other hand, reducible errors, are errors that can be reduced to improve the model&rsquo;s skill. Bias and Variance are two of the latter. They are concepts used in supervised Machine Learning to evaluate the model&rsquo;s output compared to the true values.">

	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

  </head>

  <body class="ma0 sans-serif bg-near-white">

    
   
  

  <header>
    <div class="bg-navy">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        <img src="/logo_weiss_qualle.png" class="w100 mw5-ns" alt="" />
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/" title="Home page">
              Home
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="Articles page">
              Articles
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/" title=" page">
              
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/content/" title="Content page">
              Content
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
    
    <a href="https://twitter.com/datamapu" target="_blank" rel="noopener" class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://www.linkedin.com/in/datamapu-ml-91a2622a3/" target="_blank" rel="noopener" class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
      
      <a href="https://twitter.com/share?url=http://localhost:1313/posts/ml_concepts/bias_variance/&amp;text=Bias%20and%20Variance" class="ananke-social-link twitter no-underline" aria-label="share on Twitter">
        
        <span class="icon"> <svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
        
      </a>
    
      
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/posts/ml_concepts/bias_variance/&amp;title=Bias%20and%20Variance" class="ananke-social-link linkedin no-underline" aria-label="share on LinkedIn">
        
        <span class="icon"> <svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
        
      </a>
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Bias and Variance</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-01-01T09:39:26+01:00">January 1, 2024</time>
      

      
      
        <span class="f6 mv4 dib tracked"> - 8 minutes read </span>
        <span class="f6 mv4 dib tracked"> - 1676 words </span>
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="introduction">Introduction</h2>
<p>In Machine Learning different error sources exist. Some errors cannot be avoided, for example, due to unknown variables in the system analyzed. These errors are called <em>irreducible errors</em>. On the other hand, <em>reducible errors</em>, are errors that can be reduced to improve the model&rsquo;s skill. <em>Bias</em> and <em>Variance</em> are two of the latter. They are concepts used in supervised Machine Learning to evaluate the model&rsquo;s output compared to the true values. For a Machine Learning model to be generalizable to new unseen data with high predictive skill, it is important that bias and variance are balanced.</p>
<h2 id="bias">Bias</h2>
<p>The <em>bias</em> in a Machine Learning model is a systematic error in the predictions due to wrong assumptions during the modeling process. It describes the deviation from the model&rsquo;s prediction to the true target data. Mathematically, the bias is defined as</p>
<p>$$Bias = E(\hat{Y}) - Y,$$</p>
<p>with $\hat{Y}$ the predictions produced by the Machine Learning model and $Y$ the true target values. That is, the bias is the difference between the expected model predictions and the true values. A bias results from assumptions that are made of the underlying data. Since every Machine Learning model is based on some assumptions, all models underly a certain bias. A <strong>low bias</strong> means fewer assumptions were made and the model fits the data well. A <strong>high bias</strong> can be introduced by using too simplified assumptions about the mapping that is supposed to be modeled, e.g. using a model that is too simple. In this case, the model is not able to capture the underlying pattern of the data. This is also known as <em>underfitting</em>.</p>
<h3 id="possibilities-to-reduce-the-bias">Possibilities to reduce the Bias</h3>
<p>In general, a low bias is desirable. There is, however, no recipe for how to reduce it. The following methods can be tried.</p>
<p><strong>Select a more complex model architecture.</strong> If the selected model is too simple compared to the underlying data, the bias will always be high. For example, if a linear model is used to model a non-linear relationship, the model will never be able to capture the underlying pattern, no matter how long and with how much data is trained.</p>
<p><strong>Increase the number of input features.</strong> More complexity can not only be introduced by the model structure itself but also by using more input features. The additional features can help to identify the modeled relationship.</p>
<p><strong>Gather more training data.</strong> A larger training dataset can help to learn the underlying pattern between input features and target data.</p>
<p><strong>Decrease regularization.</strong> Regularization techniques are used to prevent overfitting and make the model more generalizable. This is useful if the model shows high variance, as we will see in the next section. However, if the bias is high reducing the regularization may help.</p>
<h2 id="variance">Variance</h2>
<p><em>Variance</em> is a term from statistics, which measures the spread of a variable around its mean. In Machine Learning it describes the change in the predictions when different subsets are used for training, or in other words the variability of the model&rsquo;s prediction. Mathematically the variance is described as the expected value of the square of the difference between the predicted values and the expected value of the predictions</p>
<p>$$Variance = E[(\hat{Y} - E[\hat{Y}])^2],$$</p>
<p>with $\hat{Y}$ the predictions produced by the Machine Learning model and $Y$ the true target values. <strong>Low variance</strong> means that the variability between the training on different subsets is low. That is the model is less sensitive to changes in the training data and able to generalize to unseen data equally well independently of the data subset it was trained on. On the other hand <strong>high variance</strong> means that the model is highly sensitive to the training data and the model results differ depending on the selected subset. High variance implies that the model fits very well to the training data, but is not able to generalize to new data. This phenomenon is called <em>overfitting</em>. High variance can result from a complex model with a large set of features.</p>
<h3 id="possibilities-to-reduce-the-variance">Possibilities to reduce the Variance</h3>
<p>To make the model generalizable to new data, a low variance is desirable. As for the bias, there is no recipe to achieve this. The following methods may help to reduce the variance.</p>
<p><strong>Select a less complex model.</strong> High variance often results from a model that is too complex, that fits the specific training data sample too well and by doing that oversees the general pattern.</p>
<p><strong>Use cross validation.</strong> In cross validation, the training data is split into different subsets that are used to train the model. Tuning the hyperparameters on different subsets can make the model more stable and reduce the variance.</p>
<p><strong>Select relevant features.</strong> Analog to reducing the bias by increasing the number of features, we can try to reduce the variance by removing features and with that reduce the complexity of the model.</p>
<p><strong>Use regularization.</strong> Regularization adds an extra term to the loss function, which is used to weigh features by their importance.</p>
<p><strong>Use ensemble models.</strong> In <a href="http://localhost:1313/posts/ml_concepts/ensemble/">Ensemble learning</a>, multiple models are used and aggregated into one single prediction. Different types of ensemble models exist, <a href="http://localhost:1313/posts/ml_concepts/ensemble/#bagging">Bagging</a> is especially suited to reduce the variance.</p>
<p>The following table shows and summarizes all possible combinations of Bias and Variance.</p>
<p><img src="/images/bias_variance/bias_variance_2.png" alt="bias and variance">
<em>Overview about the combinations of bias and variance.</em></p>
<p>The effect of Bias and Variance is often illustrated using a dartboard as shown in the following plot.</p>
<p><img src="/images/bias_variance/bias_variance_3.png" alt="bias and variance">
<em>Illustration of the combinations of bias and variance.</em></p>
<h2 id="tradeoff">Bias-Variance Tradeoff</h2>
<p>Concluding the above derivations, it is in general desirable to achieve a low bias as well as a low variance. This is however difficult. Intuitively this is clear because a model cannot be simple and complex at the same time. Mathematically, the bias and the variance are part of the total error of a Machine Learning model. Let $Y$ be the true values and $\hat{Y}$ the model&rsquo;s estimates with $Y = \hat{Y} + \epsilon$ and $\epsilon$ a normally distributed error with mean $0$ and standard deviation $\sigma$. The predictions $\hat{Y}$ depend on the dataset the model has been trained on, while the true values $Y$ are independent of the specific dataset. The expected (squared) error, that is aimed to be minimized can then be written as</p>
<p>$$E[(Y - \hat{Y})^2] = E[Y^2 - 2Y\hat{Y} + \hat{Y}^2].$$</p>
<p>Because of the linearity of the expected value this equal to</p>
<p>$$E[(Y - \hat{Y})^2] = E[Y^2] - 2E[Y\hat{Y}] + E[\hat{Y}^2]. (1)$$</p>
<p>Let&rsquo;s consider these three terms individually. We can reformulate $E[Y^2]$ as follows</p>
<p>$$E[Y^2] = E[(\hat{Y} + \epsilon)^2] = E[Y^2] + 2E[Y\epsilon] + E[\epsilon^2].$$</p>
<p>Since the true values $Y$ are independent of the dataset, this is equal to</p>
<p>$$E[Y^2] = Y^2 + 2YE[\epsilon] + E[\epsilon^2].$$</p>
<p>We can use the following formula of the <a href="https://en.wikipedia.org/wiki/Variance">variance</a> for a variable $X$</p>
<p>$$Var[X] = E[(X - E[X])^2]$$</p>
<p>and reformulate it as</p>
<p>$$Var[X] = E[X^2 - 2XE[X] + E[X]^2]$$
$$Var[X] = E[X^2] - 2E[X]E[X] + E[X]^2$$
$$Var[X] = E[X^2] - E[X]^2$$</p>
<p>to rewrite this equation. Since we assumed $\epsilon$ to have mean $0$ and standard deviation $\sigma$ the term simplifies to</p>
<p>$$E[Y^2] = Y^2 + 2YE[\epsilon] + Var[\epsilon] + E[\epsilon^2] = Y^2 + \sigma^2. (2)$$</p>
<p>Using the equation for the variance, the last term of equation (1) can be written as</p>
<p>$$E[\hat{Y}^2] = E[\hat{Y}^2] - E[\hat{Y}]^2 + E[\hat{Y}]^2 = Var[\hat{Y}] + E[\hat{Y}]^2. (3)$$</p>
<p>Putting (2) and (3) back into equation (1) and using the indepence of $Y$ of the dataset, leads to</p>
<p>$$E[(Y - \hat{Y})^2)] = Y^2 + \sigma^2 - 2YE[\hat{Y}] + Var[\hat{Y}] + E[\hat{Y}]^2.$$</p>
<p>This can be written as</p>
<p>$$E[(Y - \hat{Y})^2] = (Y - E[\hat{Y}])^2 + Var[\hat{Y}] + \sigma^2.$$</p>
<p>In other words the total error in a Machine Learning model is</p>
<p>$$E[(Y - \hat{Y})^2] = Bias^2 + Variance + \sigma^2,$$</p>
<p>with $\sigma$ being the irreducible error. <strong>The total error of a Machine Learning Model is thus composed of the Bias, the Variance, and the irreducible error.</strong> The difficulty of minimizing both bias and variance to find a good balance such that the model does not overfit and not underfit is known as the <em>Bias-Variance Tradeoff</em>. It can be illustrated as follows.</p>
<p><img src="/images/bias_variance/bias_variance_1.png" alt="bias and variance">
<em>Underfitting and Overfitting illustrated for a regression problem.</em></p>
<p><img src="/images/bias_variance/bias_variance_4.png" alt="bias and variance">
<em>Underfitting and overfitting illustrated for a classification problem.</em></p>
<p>The relationship between the general error, Bias, and Variance can be illustrated as follows</p>
<p><img src="/images/bias_variance/Bias_and_variance_contributing_to_total_error.svg" alt="bias and variance">
<em><a href="https://commons.wikimedia.org/w/index.php?curid=105307219">https://commons.wikimedia.org/w/index.php?curid=105307219</a> (Von Bigbossfarin - Eigenes Werk, CC0)</em></p>
<h2 id="example">Example</h2>
<p>To illustrate the modelling of simple to complex models, we consider an example of a polynomial function, which is modeled by polynoms of different degrees.</p>
<div class="highlight"><div style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">28
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#179299">import</span> <span style="color:#fe640b">numpy</span> <span style="color:#8839ef">as</span> <span style="color:#fe640b">np</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig, ax <span style="color:#04a5e5;font-weight:bold">=</span> plt<span style="color:#04a5e5;font-weight:bold">.</span>subplots(<span style="color:#fe640b">3</span>, <span style="color:#fe640b">2</span>, figsize<span style="color:#04a5e5;font-weight:bold">=</span>(<span style="color:#fe640b">12</span>,<span style="color:#fe640b">16</span>))
</span></span><span style="display:flex;"><span>np<span style="color:#04a5e5;font-weight:bold">.</span>random<span style="color:#04a5e5;font-weight:bold">.</span>seed(<span style="color:#fe640b">42</span>)
</span></span><span style="display:flex;"><span>x <span style="color:#04a5e5;font-weight:bold">=</span> np<span style="color:#04a5e5;font-weight:bold">.</span>arange(<span style="color:#fe640b">0</span>,<span style="color:#fe640b">30</span>,<span style="color:#fe640b">1</span>)
</span></span><span style="display:flex;"><span>noise <span style="color:#04a5e5;font-weight:bold">=</span> np<span style="color:#04a5e5;font-weight:bold">.</span>random<span style="color:#04a5e5;font-weight:bold">.</span>normal(<span style="color:#fe640b">0</span>, <span style="color:#fe640b">500</span>,x<span style="color:#04a5e5;font-weight:bold">.</span>shape[<span style="color:#fe640b">0</span>])
</span></span><span style="display:flex;"><span>a0 <span style="color:#04a5e5;font-weight:bold">=</span> <span style="color:#fe640b">20</span>
</span></span><span style="display:flex;"><span>a1 <span style="color:#04a5e5;font-weight:bold">=</span> <span style="color:#fe640b">0.3</span>
</span></span><span style="display:flex;"><span>a2 <span style="color:#04a5e5;font-weight:bold">=</span> <span style="color:#04a5e5;font-weight:bold">-</span><span style="color:#fe640b">25</span>
</span></span><span style="display:flex;"><span>a3 <span style="color:#04a5e5;font-weight:bold">=</span> <span style="color:#fe640b">1.1</span>
</span></span><span style="display:flex;"><span>y <span style="color:#04a5e5;font-weight:bold">=</span> a0 <span style="color:#04a5e5;font-weight:bold">+</span> a1<span style="color:#04a5e5;font-weight:bold">*</span>x <span style="color:#04a5e5;font-weight:bold">+</span> a2<span style="color:#04a5e5;font-weight:bold">*</span>x<span style="color:#04a5e5;font-weight:bold">**</span><span style="color:#fe640b">2</span> <span style="color:#04a5e5;font-weight:bold">+</span> a3<span style="color:#04a5e5;font-weight:bold">*</span>x<span style="color:#04a5e5;font-weight:bold">*</span>x<span style="color:#04a5e5;font-weight:bold">*</span>x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># training data</span>
</span></span><span style="display:flex;"><span>y1 <span style="color:#04a5e5;font-weight:bold">=</span> y <span style="color:#04a5e5;font-weight:bold">+</span> noise
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># models</span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># 1. linear</span>
</span></span><span style="display:flex;"><span>model1 <span style="color:#04a5e5;font-weight:bold">=</span> np<span style="color:#04a5e5;font-weight:bold">.</span>poly1d(np<span style="color:#04a5e5;font-weight:bold">.</span>polyfit(x, y1, <span style="color:#fe640b">1</span>))
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># 2. quadratic</span>
</span></span><span style="display:flex;"><span>model2 <span style="color:#04a5e5;font-weight:bold">=</span> np<span style="color:#04a5e5;font-weight:bold">.</span>poly1d(np<span style="color:#04a5e5;font-weight:bold">.</span>polyfit(x, y1, <span style="color:#fe640b">2</span>))
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># 3. cubic</span>
</span></span><span style="display:flex;"><span>model3 <span style="color:#04a5e5;font-weight:bold">=</span> np<span style="color:#04a5e5;font-weight:bold">.</span>poly1d(np<span style="color:#04a5e5;font-weight:bold">.</span>polyfit(x, y1, <span style="color:#fe640b">3</span>))
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># 4. degree 10</span>
</span></span><span style="display:flex;"><span>model4 <span style="color:#04a5e5;font-weight:bold">=</span> np<span style="color:#04a5e5;font-weight:bold">.</span>poly1d(np<span style="color:#04a5e5;font-weight:bold">.</span>polyfit(x, y1, <span style="color:#fe640b">10</span>))
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># 5. degree 15</span>
</span></span><span style="display:flex;"><span>model5 <span style="color:#04a5e5;font-weight:bold">=</span> np<span style="color:#04a5e5;font-weight:bold">.</span>poly1d(np<span style="color:#04a5e5;font-weight:bold">.</span>polyfit(x, y1, <span style="color:#fe640b">15</span>))
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># 6. degree 20</span>
</span></span><span style="display:flex;"><span>model6 <span style="color:#04a5e5;font-weight:bold">=</span> np<span style="color:#04a5e5;font-weight:bold">.</span>poly1d(np<span style="color:#04a5e5;font-weight:bold">.</span>polyfit(x, y1, <span style="color:#fe640b">20</span>))
</span></span></code></pre></td></tr></table>
</div>
</div><p>Plotting the different models against the &ldquo;true model&rdquo; $y = 20 + 0.3x - 25x^2 + 1.1 x^3$ shows how simple models as the linear one show a high bias and complex models as the one of degree $20$ show a high variance. The cubic model fits the true model best.</p>
<p><img src="/images/bias_variance/model_complexity.png" alt="bias and variance">
<em>Example for polynomial models of different degrees.</em></p>
<h2 id="summary">Summary</h2>
<p>Bias and Variance are different types of errors in Machine Learning. Both high bias and high variance mean that a model is not able to understand the underlying pattern of the data and is therefore not able to generalize to new unseen data. In practice, it is important to balance bias and variance to achieve a good model.</p>
<h2 id="further-reading">Further Reading</h2>
<p>Emmert-Streib F. and Dehmert M., &ldquo;Evaluation of Regression Models: Model Assessment,
Model Selection and Generalization Error&rdquo;, machine learning and knowledge extraction (2019), DOI: 10.3390/make1010032</p>
<hr>
<p>If this blog is useful for you, please consider supporting.</p>



<a class="hugo-shortcodes-bmc-button" href="https://www.buymeacoffee.com/pumaline">
    <img src="https://img.buymeacoffee.com/button-api/?button_colour=ffdd00&amp;coffee_colour=ffffff&amp;emoji=&amp;font_colour=000000&amp;font_family=Cookie&amp;outline_colour=000000&amp;slug=pumaline&amp;text=Buy&#43;me&#43;a&#43;coffee" alt="Buy me a coffee" />
</a>

<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/data-science/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Data Science</a>
   </li>
  
   <li class="list di">
     <a href="/tags/machine-learning/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Machine Learning</a>
   </li>
  
   <li class="list di">
     <a href="/tags/artificial-intelligence/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Artificial Intelligence</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/posts/deep_learning/intro_dl/">Introduction to Deep Learning</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/ensemble/">Ensemble Models - Illustrated</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/random_forest/">Random Forests - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/decision_tree_regression_example/">Decision Trees for Regression - Example</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/decision_tree_classification_example/">Decision Trees for Classification - Example</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/decision_trees/">Decision Trees - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/feature_selection/">Feature Selection Methods</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/logistic_regression/">Logistic Regression - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/linear_regression_example/">Linear Regression - Analytical Solution and Simplified Example</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/linear_regression/">Linear Regression - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/supervised_unsupervised/">Supervised versus Unsupervised Learning - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/classification_metrics/">Metrics for Classification Problems</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/regression_metrics/">Metrics for Regression Problems</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/datascience_lifecycle/">The Data Science Lifecycle</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-navy bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy; 
  </a>
    <div>
<div class="ananke-socials">
  
    
    <a href="https://twitter.com/datamapu" target="_blank" rel="noopener" class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://www.linkedin.com/in/datamapu-ml-91a2622a3/" target="_blank" rel="noopener" class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
