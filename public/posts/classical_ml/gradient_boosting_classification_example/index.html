<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=40185&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Gradient Boost for Classification Example | </title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Introduction In this post, we develop a Gradient Boosting model for a binary classification. We focus on the calculations of each single step for a specific example chosen. For a more general explanation of the algorithm and the derivation of the formulas for the individual steps, please refer to Gradient Boost for Classification - Explained and Gradient Boost for Regression - Explained. Additionally, we show a simple example of how to apply Gradient Boosting for classification in Python.">
    <meta name="generator" content="Hugo 0.125.6">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />


    

    
    
    <meta property="og:url" content="http://localhost:40185/posts/classical_ml/gradient_boosting_classification_example/">
  <meta property="og:title" content="Gradient Boost for Classification Example">
  <meta property="og:description" content="Introduction In this post, we develop a Gradient Boosting model for a binary classification. We focus on the calculations of each single step for a specific example chosen. For a more general explanation of the algorithm and the derivation of the formulas for the individual steps, please refer to Gradient Boost for Classification - Explained and Gradient Boost for Regression - Explained. Additionally, we show a simple example of how to apply Gradient Boosting for classification in Python.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-28T17:01:32-03:00">
    <meta property="article:modified_time" content="2024-04-28T17:01:32-03:00">
    <meta property="article:tag" content="Data Science">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Gradient Boosting">
    <meta property="article:tag" content="Ensemble">
    <meta property="article:tag" content="Boosting">
    <meta property="article:tag" content="Tree Methods">
    <meta property="og:image" content="http://localhost:40185/images/gradient_boosting/gb_clf_final.png">

  <meta itemprop="name" content="Gradient Boost for Classification Example">
  <meta itemprop="description" content="Introduction In this post, we develop a Gradient Boosting model for a binary classification. We focus on the calculations of each single step for a specific example chosen. For a more general explanation of the algorithm and the derivation of the formulas for the individual steps, please refer to Gradient Boost for Classification - Explained and Gradient Boost for Regression - Explained. Additionally, we show a simple example of how to apply Gradient Boosting for classification in Python.">
  <meta itemprop="datePublished" content="2024-04-28T17:01:32-03:00">
  <meta itemprop="dateModified" content="2024-04-28T17:01:32-03:00">
  <meta itemprop="wordCount" content="1548">
  <meta itemprop="image" content="http://localhost:40185/images/gradient_boosting/gb_clf_final.png">
  <meta itemprop="keywords" content="Data Science,Machine Learning,Gradiend Boosting,Ensemble,Classification,Tree Methods"><meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:40185/images/gradient_boosting/gb_clf_final.png"><meta name="twitter:title" content="Gradient Boost for Classification Example">
<meta name="twitter:description" content="Introduction In this post, we develop a Gradient Boosting model for a binary classification. We focus on the calculations of each single step for a specific example chosen. For a more general explanation of the algorithm and the derivation of the formulas for the individual steps, please refer to Gradient Boost for Classification - Explained and Gradient Boost for Regression - Explained. Additionally, we show a simple example of how to apply Gradient Boosting for classification in Python.">

	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

  </head>

  <body class="ma0 sans-serif bg-near-white">

    
   
  

  <header>
    <div class="bg-navy">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        <img src="/logo_weiss_qualle.png" class="w100 mw5-ns" alt="" />
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/" title="Home page">
              Home
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="Articles page">
              Articles
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/" title=" page">
              
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/content/" title="Content page">
              Content
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
    
    <a href="https://twitter.com/datamapu" target="_blank" rel="noopener" class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://www.linkedin.com/in/datamapu-ml-91a2622a3/" target="_blank" rel="noopener" class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
      
      <a href="https://twitter.com/share?url=http://localhost:40185/posts/classical_ml/gradient_boosting_classification_example/&amp;text=Gradient%20Boost%20for%20Classification%20Example" class="ananke-social-link twitter no-underline" aria-label="share on Twitter">
        
        <span class="icon"> <svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
        
      </a>
    
      
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:40185/posts/classical_ml/gradient_boosting_classification_example/&amp;title=Gradient%20Boost%20for%20Classification%20Example" class="ananke-social-link linkedin no-underline" aria-label="share on LinkedIn">
        
        <span class="icon"> <svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
        
      </a>
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Gradient Boost for Classification Example</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-04-28T17:01:32-03:00">April 28, 2024</time>
      

      
      
        <span class="f6 mv4 dib tracked"> - 8 minutes read </span>
        <span class="f6 mv4 dib tracked"> - 1548 words </span>
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="introduction">Introduction</h2>
<p>In this post, we develop a Gradient Boosting model for a binary classification. We focus on the calculations of each single step for a specific example chosen. For a more general explanation of the algorithm and the derivation of the formulas for the individual steps, please refer to <a href="http://localhost:40185/posts/classical_ml/gradient_boosting_classification/">Gradient Boost for Classification - Explained</a> and <a href="http://localhost:40185/posts/classical_ml/gradient_boosting_regression/">Gradient Boost for Regression - Explained</a>. Additionally, we show a simple example of how to apply Gradient Boosting for classification in Python.</p>
<h2 id="data">Data</h2>
<p>The dataset used for this example contains 10 samples and describes the task of whether a person should go rock climbing depending on their age, and whether the person likes height and goats. The same dataset was used in the articles <a href="http://localhost:40185/posts/classical_ml/decision_tree_classification_example/">Decision Trees for Classification - Example</a>, and <a href="http://localhost:40185/posts/classical_ml/adaboost_example_clf/">Adaboost for Classification - Example</a>.</p>
<p><img src="/images/gradient_boosting/gb_class_data.png" alt="&ldquo;data&rdquo;">
<em>The data used in this post.</em></p>
<h2 id="build-the-model">Build the Model</h2>
<p>We build a Gradient Boosting model using pruned Decision Trees as weak learners for the above dataset. The steps to be performed are summarized in the plot below. For a more detailed explanation please refer to <a href="http://localhost:40185/posts/classical_ml/gradient_boosting_classification/">Gradient Boost for Classification - Explained</a>.</p>
<p><img src="/images/gradient_boosting/gradient_boosting_class.png" alt="Gradient Boosting for Classification">
<em>Gradient Boosting Algorithm simplified for a binary classification task.</em></p>
<h4 id="step-1---initialize-the-model-with-a-constant-value---f_0x--logbigfracp1-pbig">Step 1 - Initialize the model with a constant value - $F_0(X) = \log\Big(\frac{p}{1-p}\Big)$.</h4>
<p>For the first initialization, we need the probability of &ldquo;go rock climbing&rdquo;. In the above dataset, we have five $1$s and five $0$s, and a total dataset size of ten samples. That is the probability of &ldquo;going rock climbing&rdquo; is $\frac{5}{10} = 0.5$. With that, we can calculate</p>
<p>$$F_0(X) = \log\Big(\frac{P(y=1)}{P(y=0)}\Big) =\log\Big(\frac{0.5}{0.5}\Big) = 0.$$</p>
<p>The initial log-loss in this case is $0$, which means that the chances of both outcomes of the target variable are equal. The initial predictions are thus
$$F_0(X) = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]$$</p>
<p>To check the performance of our model, we use the accuracy score. We calculate the accuracy after each step, to see how the model evolves. For the initial predictions, we first calculate the probabilities.</p>
<p>$$p_0 = \frac{1}{1 + e^{-F_0(X)}} = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$$</p>
<p>As a threshold for predicting a $1$ we use $0.5$. In this case, the predictions are</p>
<p>$$\hat{y}_0 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]$$.</p>
<p>The <a href="http://localhost:40185/posts/ml_concepts/classification_metrics/#metrics">accuracy</a> is calculated as the fraction of the correct predicted values and the total number of samples</p>
<p>$$accuracy = \frac{5}{10} = 0.5$$</p>
<h4 id="step-2---for-m1-to-m2">Step 2 - For $m=1$ to $M=2$:</h4>
<p>In this step we sequentially fit weak learners (pruned Decision Trees) to the residuals. The number of loops is the number of weak learners considered. Because the data considered in this post is so simple, we will only loop twice, i.e.
.</p>
<h4 id="first-loop-m1">First loop $M=1$</h4>
<p>Fit the first Decision Tree.</p>
<h4 id="2a-compute-the-residuals-of-the-preditions-and-the-true-observations">2A. Compute the residuals of the preditions and the true observations.</h4>
<p>The residuals are calculated as</p>
<p>$$r_1 = y - p_0 = [-0.5, 0.5, 0.5, -0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5]$$</p>
<h4 id="2b-and-2c-fit-a-model-weak-learner-to-the-residuals-and-find-the-optimized-solution">2B. and 2C. Fit a model (weak learner) to the residuals and find the optimized solution.</h4>
<p>In this step, we use the above calculated residuals to fit a Decision Tree. We prune the tree to use it as a weak learner. We use a maximum depth of two. We don&rsquo;t develop the Decision Tree by hand, but will use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">sklearn</a> method <em>DecisionTreeClassifier</em>. For a step-by-step example of building a Decision Tree for Classification, please refer to the article <a href="http://localhost:40185/posts/classical_ml/decision_tree_classification_example/">Decision Trees for Classification - Example</a>.</p>
<p><img src="/images/gradient_boosting/gb_first_tree_clf.png" alt="&ldquo;first tree&rdquo;">
<em>First Decision Tree, i.e. first weak learner</em></p>
<p>The resulting predictions are</p>
<p>$$pred_1(X) = [-0.5,  0.5,  0. , -0.5, -0.5,  0.5, -0.5,  0.5,  0. ,  0.5]$$</p>
<h4 id="2d-update-the-model">2D. Update the model.</h4>
<p>We update the model using the predictions of the above developed Decision Tree. We use a learning rate of $\alpha = 0.3$ for this example. This results in the updated model</p>
<p>$$F_1(X) = F_0(X) + 0.3 * pred_1(X).$$</p>
<p>Using the calculated numbers, this leads to</p>
<p>$$F_1(X) = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] + 0.3 \cdot [-0.5,  0.5,  0. , -0.5, -0.5,  0.5, -0.5,  0.5,  0. ,  0.5]$$
$$F_1(X) = [-0.15, 0.15, 0, -0.15, -0.15, 0.15, -0.15, 0.15, 0, 0.15]$$</p>
<p>The probabilies resulting from these log-loss are</p>
<p>$$p_1 = \frac{1}{1 + e^{-F_1(X)}} = [0.46, 0.54, 0.5, 0.46, 0.46, 0.54, 0.46, 0.54, 0.5, 0.54].$$</p>
<p>With the threshold of $0.5$ this leads to the predictions</p>
<p>$$\hat{y}_2(X) = [0, 1, 1, 0, 0, 1, 0, 1, 1, 1].$$</p>
<p>The accuracy after this step is</p>
<p>$$accuracy = \frac{9}{10} = 0.9.$$</p>
<h4 id="second-loop-m2">Second loop $M=2$</h4>
<p>Fit the second Decision Tree.</p>
<h4 id="2a-compute-the-residuals-of-the-preditions-and-the-true-observations-1">2A. Compute the residuals of the preditions and the true observations</h4>
<p>The residuals are calculated as</p>
<p>$$r_2 = y - p_1 = [-0.46, 0.46, 0.5, -0.46, -0.46, 0.46, -0.46,  0.46, -0.5,  0.46]$$</p>
<h4 id="2b-and-2c-fit-a-model-weak-learner-to-the-residuals-and-find-the-optimized-solution-1">2B. and 2C. Fit a model (weak learner) to the residuals and find the optimized solution.</h4>
<p>Now we fit the second Decision Tree with the residuals $r_2$ calculated above as target.</p>
<p><img src="/images/gradient_boosting/gb_second_tree_clf.png" alt="&ldquo;second tree&rdquo;">
<em>Second Decision Tree, i.e. second weak learner</em></p>
<p>The resulting predictions are</p>
<p>$$pred_2(X) = [-0.46,  0.46,  0, -0.46, -0.46, 0.46, -0.46,  0.46,  0,  0.46]$$</p>
<h4 id="2d-update-the-model-1">2D. Update the model.</h4>
<p>We again update the model using the predictions of this second Decision Tree with a learning rate of $0.3$. This results in the updated model</p>
<p>$$F_2(X) = F_1(X) + 0.3 \cdot pred_2(X).$$</p>
<p>Using the calculated numbers, this leads to</p>
<p>$$F_2(X) = [-0.15, 0.15, 0, -0.15, -0.15, 0.15, -0.15, 0.15, 0, 0.15] + $$
$$0.3 \cdot [-0.46,  0.46,  0, -0.46, -0.46, 0.46, -0.46,  0.46,  0,  0.46]$$
$$F_2(X) = [-0.29,  0.29,  0, -0.29, -0.29, 0.29, -0.29,  0.29,  0, 0.29]$$</p>
<p>The probabilies resulting from these log-loss are</p>
<p>$$p_2 = \frac{1}{1 + e^{-F_2(X)}} = [0.43, 0.57, 0.5, 0.43, 0.43, 0.57, 0.43, 0.57, 0.5, 0.57].$$</p>
<p>With the threshold of $0.5$ this leads to the predictions</p>
<p>$$\hat{y}_3 = [0, 1, 1, 0, 0, 1, 0, 1, 1, 1].$$</p>
<p>The accuracy after this step is</p>
<p>$$accuracy = \frac{9}{10} = 0.9.$$</p>
<p>That is, this second step didn&rsquo;t improve the accuracy of our model.</p>
<h4 id="step-3---output-final-model-f_mx">Step 3 - Output final model $F_M(x)$.</h4>
<p>The final model then in defined by the output of the last step</p>
<p>$$F_2(X) = F_0(X) + \alpha \cdot F_1(X) + \alpha \cdot pred_2.$$</p>
<p>Using the input features $X$ given in the above table, this is</p>
<p>$$F_2(X) = [-0.29,  0.29,  0, -0.29, -0.29, 0.29, -0.29,  0.29,  0, 0.29].$$</p>
<p><img src="/images/gradient_boosting/gb_example_clf_final.png" alt="&ldquo;final model&rdquo;">
<em>Final model.</em></p>
<p>Accordingly the final probabilities $p_2$ and predictions $\hat{y}_3$ as calculated in the last step of the above loop.</p>
<h2 id="fit-a-model-in-python">Fit a Model in Python</h2>
<p>In this section, we will show how to fit a <em>GradientBoostingClassifier</em> from the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">sklearn</a> library to the above data. In sklearn the weak learners are fixed to Decision Trees and cannot be changed. The <em>GradientBoostingClassifier</em> offers a set of hyperparameters, that can be changed and tested to improve the model. Note, that we are considering a very simplified example. In this case, we set the number of weak learners (<em>n_estimators</em>) to $2$, the maximal depth (<em>max_depth</em>) of the Decision Trees to $2$, and the <em>learning_rate</em>=$0.3$. For real-world data, usually, hyperparameters are chosen differently, the default values in sklearn are <em>n_estimators</em>$=100$, <em>max_depth</em>$=3$, and the <em>learning_rate</em>=$0.1$. For a complete list of available hyperparameters, please refer to the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">sklearn documentation</a>.</p>
<p>We read the data into a Pandas dataframe.</p>
<div class="highlight"><div style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#179299">import</span> <span style="color:#fe640b">pandas</span> <span style="color:#8839ef">as</span> <span style="color:#fe640b">pd</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#04a5e5;font-weight:bold">=</span> {<span style="color:#40a02b">&#39;age&#39;</span>: [<span style="color:#fe640b">23</span>, <span style="color:#fe640b">31</span>, <span style="color:#fe640b">35</span>, <span style="color:#fe640b">35</span>, <span style="color:#fe640b">42</span>, <span style="color:#fe640b">43</span>, <span style="color:#fe640b">45</span>, <span style="color:#fe640b">46</span>, <span style="color:#fe640b">46</span>, <span style="color:#fe640b">51</span>],
</span></span><span style="display:flex;"><span>        <span style="color:#40a02b">&#39;likes goats&#39;</span>: [<span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>],
</span></span><span style="display:flex;"><span>        <span style="color:#40a02b">&#39;likes height&#39;</span>: [<span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">1</span>],
</span></span><span style="display:flex;"><span>        <span style="color:#40a02b">&#39;go rock climbing&#39;</span>: [<span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>, <span style="color:#fe640b">0</span>, <span style="color:#fe640b">1</span>]}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#04a5e5;font-weight:bold">=</span> pd<span style="color:#04a5e5;font-weight:bold">.</span>DataFrame(data)
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now we can fit the model using the above defined hyperparameters.</p>
<div class="highlight"><div style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#179299">from</span> <span style="color:#fe640b">sklearn.ensemble</span> <span style="color:#179299">import</span> GradientBoostingClassifier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X <span style="color:#04a5e5;font-weight:bold">=</span> df[[<span style="color:#40a02b">&#39;age&#39;</span>, <span style="color:#40a02b">&#39;likes goats&#39;</span>, <span style="color:#40a02b">&#39;likes height&#39;</span>]]<span style="color:#04a5e5;font-weight:bold">.</span>values
</span></span><span style="display:flex;"><span>y <span style="color:#04a5e5;font-weight:bold">=</span> df[[<span style="color:#40a02b">&#39;go rock climbing&#39;</span>]]<span style="color:#04a5e5;font-weight:bold">.</span>values<span style="color:#04a5e5;font-weight:bold">.</span>reshape(<span style="color:#04a5e5;font-weight:bold">-</span><span style="color:#fe640b">1</span>,)
</span></span><span style="display:flex;"><span>clf <span style="color:#04a5e5;font-weight:bold">=</span> GradientBoostingClassifier(
</span></span><span style="display:flex;"><span>    n_estimators<span style="color:#04a5e5;font-weight:bold">=</span><span style="color:#fe640b">2</span>,
</span></span><span style="display:flex;"><span>    max_depth<span style="color:#04a5e5;font-weight:bold">=</span><span style="color:#fe640b">2</span>,
</span></span><span style="display:flex;"><span>    learning_rate<span style="color:#04a5e5;font-weight:bold">=</span><span style="color:#fe640b">0.3</span>	
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>clf<span style="color:#04a5e5;font-weight:bold">.</span>fit(X, y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>The <em>GradientBoostingClassifier</em> has a <em>predict</em> method, which we can use to get the predictions and the <em>score</em> method to calculate the accuracy.</p>
<div class="highlight"><div style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#8c8fa1">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span>y_pred <span style="color:#04a5e5;font-weight:bold">=</span> clf<span style="color:#04a5e5;font-weight:bold">.</span>predict(X)
</span></span><span style="display:flex;"><span>score <span style="color:#04a5e5;font-weight:bold">=</span> clf<span style="color:#04a5e5;font-weight:bold">.</span>score(X, y)
</span></span></code></pre></td></tr></table>
</div>
</div><p>We get the predictions $[0, 1, 0, 0, 0, 1, 0, 1, 0, 1]$ and a score of $0.9$. A more detailed example of applying Gradient Boosting in Python can be found on <a href="https://www.kaggle.com/code/pumalin/gradient-boosting-tutorial">kaggle</a> - this is an example of a regression problem. The application to a classification problem, however, is the same, we only need to change the <em>GradientBoostingRegressor</em> to the <em>GradientBoostingClassifier</em> and change the metrics to evaluate the model.</p>
<h2 id="summary">Summary</h2>
<p>In this post, we went through the calculations of the individual steps to build a Gradient Boosting model for a binary classification problem. A simple example was chosen to make explicit calculations feasible. Later, we showed how to fit a model using Python. In real-world examples, libraries like <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">sklearn</a> are used to develop such models. The development is usually not as straightforward as in this simplified example, but a set of hyperparameters are (systematically) tested to optimize the solution. For a more detailed explanation of the algorithm, please refer to the related articles <a href="http://localhost:40185/posts/classical_ml/gradient_boosting_classification/">Gradient Boost for Classification - Explained</a>, <a href="http://localhost:40185/posts/classical_ml/gradient_boosting_regression/">Gradient Boost for Regression - Explained</a>, <a href="http://localhost:40185/posts/classical_ml/gradient_boosting_regression_example/">Gradient Boost for Regression - Example</a>. For a more practical approach, please check the notebook on <a href="https://www.kaggle.com/code/pumalin/gradient-boosting-tutorial">kaggle</a>, where a Gradient Boosting model for a regression problem is developed.</p>
<hr>
<p>If this blog is useful for you, please consider supporting.</p>



<a class="hugo-shortcodes-bmc-button" href="https://www.buymeacoffee.com/pumaline">
    <img src="https://img.buymeacoffee.com/button-api/?button_colour=ffdd00&amp;coffee_colour=ffffff&amp;emoji=&amp;font_colour=000000&amp;font_family=Cookie&amp;outline_colour=000000&amp;slug=pumaline&amp;text=Buy&#43;me&#43;a&#43;coffee" alt="Buy me a coffee" />
</a>

<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/data-science/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Data Science</a>
   </li>
  
   <li class="list di">
     <a href="/tags/machine-learning/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Machine Learning</a>
   </li>
  
   <li class="list di">
     <a href="/tags/gradient-boosting/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Gradient Boosting</a>
   </li>
  
   <li class="list di">
     <a href="/tags/ensemble/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Ensemble</a>
   </li>
  
   <li class="list di">
     <a href="/tags/boosting/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Boosting</a>
   </li>
  
   <li class="list di">
     <a href="/tags/tree-methods/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Tree Methods</a>
   </li>
  
   <li class="list di">
     <a href="/tags/classification/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Classification</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/posts/classical_ml/gradient_boosting_classification/">Gradient Boost for Classification - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/gradient_boosting_regression_example/">Gradient Boost for Regression - Example</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/gradient_boosting_regression/">Gradient Boost for Regression - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/adaboost_example_reg/">Adaboost for Regression - Example</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/adaboost_example_clf/">AdaBoost for Classification - Example</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/adaboost/">AdaBoost - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/random_forest/">Random Forests - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/decision_tree_classification_example/">Decision Trees for Classification - Example</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/decision_trees/">Decision Trees - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/ensemble/">Ensemble Models - Illustrated</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/decision_tree_regression_example/">Decision Trees for Regression - Example</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/feature_selection/">Feature Selection Methods</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/classical_ml/logistic_regression/">Logistic Regression - Explained</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/ml_concepts/classification_metrics/">Metrics for Classification Problems</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/deep_learning/backpropagation/">Backpropagation Step by Step</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-navy bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:40185/" >
    &copy; 
  </a>
    <div>
<div class="ananke-socials">
  
    
    <a href="https://twitter.com/datamapu" target="_blank" rel="noopener" class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
    
    <a href="https://www.linkedin.com/in/datamapu-ml-91a2622a3/" target="_blank" rel="noopener" class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">
      
        <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span>
      
<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000"  xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;"/>
</svg>
</span></a>
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
