+++
title = 'Random Forests - Explained'
date = 2023-12-26T10:57:13+01:00
draft = true
featured_image = ''
tags = ["Data Science", "Machine Learning", "Random Forest", "Tree Methods", "Regression", "Classification"]
categories = ["Data Science", "Machine Learning", "Random Forest", "Tree Methods", "Classification", "Regression"]
keywords = ["Data Science", "Machine Learning", "Random Forest", "Tree Methods", "Classification", "Regression"]
+++

## Introduction

Random Forests are a supervised Machine Learning model, that is build on Decision Trees. A main disadvantage of [Decision Trees]() is that they tend to overfit and often have difficuilties to generalize to new data. Random Forests try to overcome this weakness. They are build of a set of Decision Trees and their outcomes are converted into a single result.

## Build a Random Forest

Random Forest consists of a number of Decision Trees. However, when building a [Decision Tree]() all possible features are considered, in a Random Forest each Tree is build of a randomly drawn subset of all features. The number of features used is one of the hyperparamters that need to be set.

< link to ensemble methods >

## Random Forests in Python

## Summary
