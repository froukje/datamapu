+++
title = 'What is Deep Learning?'
date = 2023-11-02T21:44:06+01:00
draft = true
featured_image = '/images/20231102_ai_ml_dl/'
tags = ["Data Science", "Machine Learning", "Deep Learning", "Artificial Intelligence"]
categories = ["Data Science", "Machine Learning", "Deep Learning", "Artificial Intelligence"]
keywords = ["Data Science", "Machine Learning", "Deep Learning", "Artificial Intelligence"]
+++


![ai_ml_dl](/images/20231102_ai_ml_dl/)
## What is Deep Learning?

In this article we will learn what Deep Learning is and understand the difference to AI and Machine Learning. Often these three terms are used interchangable, they are however not the same. The following diagram illustrates how they are related.


![ai_ml_dl](/images/20231102_ai_ml_dl/ai_ml_dl.png)
*Relation of Artificial Intelligence, Machine Learning and Deep Learning.*

**Artificial Intelligence.** There are different definitions of Artificial Intelligence, but in general, they involve computers performing tasks, that are usually associated with humans or other intelligent living systems. This is the definition given on Wikimedia:
It says: “[AI is] Mimicking the intelligence or behavioral pattern of humans or any living entity”

**Machine Learning.** Machine learning is a subset of Artificial Intelligence. It describes the process that a computer can “learn” from data, without a given set of rules. In practice, these terms are often not well distinguished. People often talk about ML, but refer to it as AI

**Deep Learning.** Deep Learning is a particular type of Machine Learning, which uses Neural Nets as models. Neural Nets was inspired by the way how neurons in our brain work. 

### Classical Programming vs. Machine Learning

In this article we are going to learn about Deep Learning, which as we just saw is a subset of Machine Learning. Machine Learning differs conceptionally from classical programming. In classical programming you use data and rules to get the answers to a problem.
In contrast in Machine Learning you use the data and the answers to a problem to get the rules. This is illustrated in the following chart.

![ai_ml_dl](/images/20231102_ai_ml_dl/classical_ml.png)
*Classical programming vs. Machine Learning.*

### What is a Neural Net?

A Neural Net a special type of Machine Learning model, which is used for Deep Learning.
It is often illustrated as shown in the next plot. The name and structure of a Neural Net are inspired by the human brain, mimicking the way that biological neurons signal to one another. However, other than inspiration and naming they are actually not very similar. A neural network consists of connected computational units, which are called neurons. 

![ai_ml_dl](/images/20231102_ai_ml_dl/neural_net.png)
*Illustration of a Neural Net.*

**Neuron**

Let's zoom in and understand, how a neuron works. The following plot illustrates a single neuron in more detail.
Each neuron has one or more floating point inputs, the so-called. input data, here shown as $x_1$ to $x_n$.
Each of these input numbers is multiplied with a weight - $w_1$ to $w_n$. 
Then the weighted sum of the inputs is taken and additionally, an extra constant weight, which is the bias term is added, here shown as $b$. These weights ($w_1$ to $w_n$) are learned during the training of the neural net. We will discuss how this works later.
Next, a non-linear function - the so-called “activation function” is applied to this sum
This is a pre-defined function and we will in the next paragraph how such an “activation function” looks like.
Finally the neuron returns an output value, which is also a floating point number

![ai_ml_dl](/images/20231102_ai_ml_dl/neuron.png)
*Illustration of one Neuron of a Neural Net.*

**Activation Function**

The purpose of the activation function is to introduce non-linearity into the output of a neuron. 
A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input, making it capable to learn and perform more complex tasks. 
Some typical activation function are shown in the next plot.

The **Binary Stepfunction** is $0$ for $x <= 0$ and $1$ elsewhere. This is one of the simplest activation functions.
A threshold value is used to decide whether a neuron is activated or not. If the input is greater than a certain threshold, the neuron is activated, else it is not activated. If it not activated the output is not passed on to the next hidden layer. The binary stepfunction can only be used for binary classification and not for multiple classification problems. Further the gradient of this activation function is $0$, which means it should not be used in a hidden layer, but only at the output layer.

The **Rectified Linear Unit**, or short **ReLU**, is $0$ for $x <= 0$ and $x$ elsewhere this is one of the most commonly used activation functions. With the ReLU function only neurons are activated, when the input of the activation function is positive. That is, not all neurons are activated at the same time. This fact makes the ReLU computationally efficient, may however also result in so-called "dead neurons" - i.e. neurons that are never activated.

The **Leaky ReLU** is similar to the ReLU but it has a small slope for negative values. This activation function is preferred, when we may have sparse gradients. The Leaky ReLU overcomes the problems of the ReLU activation unctions, that some neurons are never activated.

The output of the **Sigmoid activation function** is between $0$ and $1$. It is preferred, when the output is a probability. The Sigmoid-activation function is differentiable with a smooth gradient. A difficulty with the gradient tends to vanish out, because the derivative of the sigmoid function is flat for values already for relatively small values like $\pm5$. Also the sigmoid function is not symmetric around $0$, which makes the training of a Neural Net more complicated. It is preferred to use for the output layer and not for hidden layers. 

Similar, but going from $-1$ to $1$ is the **Hyperbolic Tangent (Tanh)**. Compared to the sigmoid function, the Tanh is centered around $0$. It is usually used in hidden layers, as it helps to center the data. It as the same issue considering vanishing gradients as the sigmoid activation function.

Also similar the **Inverse Tangent function (ArcTan)**, which goes from $-π/2$ to $π/2$. It has the same advantages and disadvantages as the Tanh activation function.

![ai_ml_dl](/images/20231102_ai_ml_dl/activation.png)
*Most common activation functions.*

**Neural Net**

Multiple neurons can be joined together by connecting the output of one neuron with the input of another. This then results in a neural net as illustrated in the next plot. The connections are associated with weights that determine the “strength” of the connection. The weights - aas mentioned earlier - are adjusted during training. Several neurons are aggregated into layers in a Neural Net, we distinguish between the “input layer” at the beginning, the “output layer” at the end, and the so-called “hidden layers” in between. In the following plot the weights are drawn associated to layers. They are however different for each connection, that is $w_1$ to $w_4$ in this illustration are vectors, that contain the weights for each connection. The signal - that is the data - travels from input to the output and passes through the so-called “hidden” layers.

![ai_ml_dl](/images/20231102_ai_ml_dl/layer.png)
*Illustration of a Neural Net with 5 layers: 1 input layer, 3 hidden layers, and 1 output layer.*

### How does a Neural Net Learn?

**Gradient Descent**

### How much Data do we need? 

